[2022-04-10 18:22:24,889][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /var/folders/41/_6lykj0s4ss209xm9mwxyb8c0000gn/T/tmp0snyec5o
[2022-04-10 18:22:24,890][torch.distributed.nn.jit.instantiator][INFO] - Writing /var/folders/41/_6lykj0s4ss209xm9mwxyb8c0000gn/T/tmp0snyec5o/_remote_module_non_sriptable.py
Epoch 0:   5%|█████                                                                                                       | 5/108 [00:01<00:26,  3.91it/s, loss=0.693, v_num=aqjk]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name      | Type     | Params
---------------------------------------
0 | model     | Musicnn  | 208 K
1 | valid_acc | Accuracy | 0
---------------------------------------
208 K     Trainable params
0         Non-trainable params
208 K     Total params
0.833     Total estimated model params size (MB)
/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.









Epoch 0:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████▏       | 100/108 [00:21<00:01,  4.66it/s, loss=0.667, v_num=aqjk]










Epoch 1:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████    | 104/108 [00:43<00:01,  2.40it/s, loss=0.659, v_num=aqjk]




















Epoch 3:  90%|████████████████████████████████████████████████████████████████████████████████████████████████           | 97/108 [01:25<00:09,  1.13it/s, loss=0.622, v_num=aqjk]





















Epoch 5:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████         | 99/108 [02:09<00:11,  1.31s/it, loss=0.604, v_num=aqjk]










Epoch 6:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████▏       | 100/108 [02:31<00:12,  1.52s/it, loss=0.602, v_num=aqjk]































Epoch 9:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████         | 99/108 [03:36<00:19,  2.18s/it, loss=0.588, v_num=aqjk]




